QDRANT_COLLECTION=knowledge_base
QDRANT_PORT=6333
QDRANT_HOST=localhost
EMBEDDING_MODEL_PATH=/mnt/dev/backend/HF_bge_large
BM25_INDEX_PATH=data/bm25_index/bm25_index.pkl
SURV_PORTAL_URL=https://survportaluat.nse.co.in/surv/document-service/document/getdocument
API_PORT=8081
USE_OLLAMA=true
RERANKER_MODEL_PATH=/mnt/dev/backend/ms-macro-MiniLm-L-6-v2

# Primary Ollama (Load Balancer)
OLLAMA_HOST=http://172.17.136.240:11434
OLLAMA_MODEL=mistral:7b-instruct

# Fallback Ollama (Direct Server - used if load balancer fails)
OLLAMA_HOST_FALLBACK=http://172.17.51.248:11434
OLLAMA_FALLBACK_TIMEOUT=10

# Parallel Processing (Optimized for Speed)
ENABLE_PARALLEL_CHUNK_PROCESSING=true
MAX_PARALLEL_CHUNKS=4
STREAMING_EXECUTOR_WORKERS=12
CHUNK_TIMEOUT_SECONDS=90

# LLM Optimization
MAX_CONCURRENT_LLM=12
OLLAMA_POOL_CONNECTIONS=30
OLLAMA_POOL_MAXSIZE=30
LLM_MAX_CHUNKS=5
LLM_ANSWER_MAX_TOKENS=1024

# Query Processing Optimization
MAX_CONCURRENT_QUERIES=40
ASYNC_EXECUTOR_MAX_WORKERS=12
QUERY_TIMEOUT_SECONDS=100

# Skip query expansion for faster response (set to false to enable expansion)
SKIP_QUERY_EXPANSION=true

# Ultra-Fast First Chunk Settings
FIRST_CHUNK_MAX_CHUNKS=2         # Use only 2 smallest chunks for first result
FIRST_CHUNK_MIN_TOKENS=256       # Min tokens for short inputs
FIRST_CHUNK_MAX_TOKENS=512       # Max tokens for long inputs
FIRST_CHUNK_TEMPERATURE=0.0      # Faster, more deterministic
